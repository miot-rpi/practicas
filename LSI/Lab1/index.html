<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../img/favicon.ico">
        <title>Laboratorio 1 - Master IoT UCM - Prácticas RPI/LSI</title>
        <link href="../../css/bootstrap-custom.min.css" rel="stylesheet">
        <link href="../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../js/bootstrap-3.0.3.min.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script> 
    </head>

    <body>

        <div class="navbar navbar-default navbar-fixed-top" role="navigation">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                        <span class="sr-only">Toggle navigation</span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </button>
                    <a class="navbar-brand" href="../..">Master IoT UCM - Prácticas RPI/LSI</a>
                </div>

                <!-- Expanded navigation -->
                <div class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li >
                                <a href="../..">Calendario</a>
                            </li>
                            <li class="dropdown active">
                                <a href="#" class="dropdown-toggle" data-toggle="dropdown">LSI <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li >
    <a href="../Lab0/">Laboratorio 0</a>
</li>
                                    
<li class="active">
    <a href="./">Laboratorio 1</a>
</li>
                                    
<li >
    <a href="../Lab2/">Laboratorio 2</a>
</li>
                                    
<li >
    <a href="../Lab3/">Laboratorio 3</a>
</li>
                                    
<li >
    <a href="../Lab4/">Laboratorio 4</a>
</li>
                                    
<li >
    <a href="../Lab5/">Laboratorio 5</a>
</li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="dropdown-toggle" data-toggle="dropdown">RPI-I <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li >
    <a href="../../RPI-I/P1/">Práctica 1</a>
</li>
                                    
<li >
    <a href="../../RPI-I/P2/">Práctica 2</a>
</li>
                                    
<li >
    <a href="../../RPI-I/P3/">Práctica 3</a>
</li>
                                    
<li >
    <a href="../../RPI-I/P4/">Práctica 4</a>
</li>
                                    
<li >
    <a href="../../RPI-I/P5/">Práctica 5</a>
</li>
                                    
<li >
    <a href="../../RPI-I/P6/">Práctica 6</a>
</li>
                                    
<li >
    <a href="../../RPI-I/P7/">Práctica 7</a>
</li>
                                    
<li >
    <a href="../../RPI-I/P8/">Práctica 8</a>
</li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="dropdown-toggle" data-toggle="dropdown">RPI-II <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li >
    <a href="../../RPI-II/P1_I/">Práctica 1 (I)</a>
</li>
                                    
<li >
    <a href="../../RPI-II/P1_II/">Práctica 1 (II)</a>
</li>
                                    
<li >
    <a href="../../RPI-II/P6/">Práctica 2 (I)</a>
</li>
                                    
<li >
    <a href="../../RPI-II/P6-II/">Práctica 2 (II)</a>
</li>
                                    
<li >
    <a href="../../RPI-II/P5/">Práctica 3</a>
</li>
                                    
<li >
    <a href="../../RPI-II/P7/">Práctica 4</a>
</li>
                                    
<li >
    <a href="../../RPI-II/P3/">Práctica 5</a>
</li>
                                    
<li >
    <a href="../../RPI-II/P10/">Práctica 6</a>
</li>
                                    
<li >
    <a href="../../RPI-II/P8/">Práctica 7</a>
</li>
                                    
<li >
    <a href="../../RPI-II/P11/">Práctica 8</a>
</li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="dropdown-toggle" data-toggle="dropdown">ANIOT <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li >
    <a href="../../ANIOT/P1/">Práctica 1</a>
</li>
                                    
<li >
    <a href="../../ANIOT/P2/">Práctica 2</a>
</li>
                                    
<li >
    <a href="../../ANIOT/P3/">Práctica 3</a>
</li>
                                    
<li >
    <a href="../../ANIOT/P4/">Práctica 4</a>
</li>
                                    
<li >
    <a href="../../ANIOT/P5/">Práctica 5</a>
</li>
                                    
<li >
    <a href="../../ANIOT/P6/">Práctica 6</a>
</li>
                                    
<li >
    <a href="../../ANIOT/P7/">Práctica 7</a>
</li>
                                    
<li >
    <a href="../../ANIOT/P8/">Práctica 8</a>
</li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav navbar-right">
                        <li>
                            <a href="#" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li >
                                <a rel="next" href="../Lab0/">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li >
                                <a rel="prev" href="../Lab2/">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
                <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
        <li class="main active"><a href="#laboratorio-1-introduccion-a-tflite-sobre-la-raspberry-pi">Laboratorio 1. Introducción a TFLite sobre la Raspberry Pi</a></li>
            <li><a href="#objetivos">Objetivos</a></li>
            <li><a href="#interaccion-con-la-camara-via-opencv">Interacción con la cámara vía OpenCV</a></li>
            <li><a href="#tensorflow-lite">TensorFlow Lite</a></li>
            <li><a href="#clasificacion-basica-de-imagenes-usando-tflite">Clasificación básica de imágenes usando TFLite</a></li>
            <li><a href="#deteccion-de-objetos-usando-tflite">Detección de objetos usando TFLite</a></li>
    </ul>
</div></div>
                <div class="col-md-9" role="main">

<h1 id="laboratorio-1-introduccion-a-tflite-sobre-la-raspberry-pi">Laboratorio 1. Introducción a TFLite sobre la Raspberry Pi</h1>
<h2 id="objetivos">Objetivos</h2>
<ul>
<li>Estudiar las APIs Python y C++ de OpenCV para realizar capturas desde cámara y para realizar transformaciones básicas en imágenes capturadas.</li>
<li>Desarrollar una aplicación básica de clasificación de imágenes combinando las APIs de OpenCV y TFLite desde Python y desde C++.</li>
<li>Acelerar el proceso de inferencia utilizando el acelerador Google Coral.</li>
<li>Diseñar e implementar una aplicación básica para detección de objetos.</li>
</ul>
<p>Puedes obtener los ficheros necesarios para el desarrollo de la práctica <a href="https://drive.google.com/file/d/14Ld3kCbXjCxA_Lax97t9Y5BKvOutR8X-/view?usp=sharing">aquí</a>.</p>
<h2 id="interaccion-con-la-camara-via-opencv">Interacción con la cámara vía OpenCV</h2>
<p>Existen múltiples APIs para acceder a la cámara proporcionada junto a la Raspberry Pi en el entorno experimental. 
<a href="https://picamera.readthedocs.io/en/release-1.13/">Picamera</a> es una API específica y de código abierto 
que proporciona una interfaz Python para el módulo de cámara de la Raspberry Pi. El problema radica en su 
especificidad para este tipo de <em>hardware</em>, que limita su aplicación o portabilidad a otros.</p>
<p>OpenCV proporciona, como parte integral de su API básica (tanto desde 
<a href="https://docs.opencv.org/master/d8/dfe/classcv_1_1VideoCapture.html">Python</a> como desde 
<a href="https://docs.opencv.org/master/d8/dfe/classcv_1_1VideoCapture.html">C++</a>)
una interfaz completa para la interacción con cámaras web, que se describe a continuación.</p>
<h3 id="captura-de-videos-desde-la-camara">Captura de vídeos desde la cámara</h3>
<p>En primer lugar, realizaremos una captura de flujo de vídeo desde la cámara, utilizando
la API de OpenCV en Python, realizando una transformación básica y mostrando el flujo de
vídeo transformado.</p>
<p>En Python, para realizar la captura de vídeo, necesitaremos un objeto de tipo
<code>VideoCapture</code>; su único argumento puede ser o bien un índice de dispositivo o un fichero
de vídeo. Un índice de dispositivo es simplemente un identificador único para cada una de las
cámaras conectadas al equipo: </p>
<pre><code class="python">import numpy as np
import cv2

# 0. Configura camara 0.
cap = cv2.VideoCapture(0)

while(True):
    # 1. Adquisicion de frame (TODO: temporizar la adquisicion y reportar Frames por Segundo (FPS)). 
    ret, frame = cap.read()

    # 2. Operaciones sobre el frame (transformaciones. TODO: investigar resize y otras transformaciones).
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

    # 3. Mostramos el frame resultante.
    cv2.imshow('frame',gray)
    if cv2.waitKey(1) &amp; 0xFF == ord('q'):
        break

# 4. Liberamos la captura y destruimos ventanas.
cap.release()
cv2.destroyAllWindows()
</code></pre>

<p><code>cap.read()</code> devuelve un valor <em>booleano</em> en función de si el <em>frame</em> fue leído correctamente o no. Podemos, 
por tanto, comprobar la finalización de un flujo de vídeo utilizando dicho valor de retorno.</p>
<p>Es posible acceder  aalgunas de las caracterísitcas del vídeo utilizando el método
<code>cap.get(propId)</code>, donde <code>propId</code> es un número entre 0 y 18. Cada número denota una 
propiedad del vídeo (si dicha propiedad se puede aplicar al vídeo en cuestión). Para más
información, consulta 
<a href="https://docs.opencv.org/master/d8/dfe/classcv_1_1VideoCapture.html#aa6480e6972ef4c00d74814ec841a2939">la página de documentación de OpenCV</a>. Algunos de estos valores pueden ser modificados a través de la función <code>cap.set(propId, value)</code>.</p>
<p>Por ejemplo, podemos comprobar la anchura y altura de un <em>frame</em> utilizando
<code>cap.get(cv.CAP_PROP_FRAME_WIDTH)</code> y <code>cap.get(cv.CAP_PROP_FRAME_HEIGHT)</code>. Podemos, por ejemplo, fijar
la resolución de la captura utilizando
<code>ret = cap.set(cv.CAP_PROP_FRAME_WIDTH,320)</code> y 
<code>ret = cap.set(cv.CAP_PROP_FRAME_HEIGHT,240)</code>.</p>
<div class="admonition danger">
<p class="admonition-title">Tarea</p>
<p>Ejecuta el anterior script (está incluido en el paquete proporcionado) utilizando <code>python3</code>. Estudia el código y modifícalo
para introducir nuevas transformaciones en las imágenes capturadas (tranformación a otros espacios de color, redimensionado 
de imágenes, etc.) Para ello, deberás consultar la documentación de OpenCV.</p>
</div>
<div class="admonition danger">
<p class="admonition-title">Tarea</p>
<p>Temporiza el tiempo de adquisición (<code>cap.read()</code>) y repórtalo a través de línea de comandos, reportando no sólo el tiempo,
sino los fotogramas por segundo (FPS) obtenidos. </p>
</div>
<p>Desde C++, la lógica de captura es muy similar, como también lo es la API utilizada:</p>
<pre><code class="cpp">// OpenCV includes.
#include &lt;opencv2/core.hpp&gt;
#include &lt;opencv2/videoio.hpp&gt;
#include &lt;opencv2/highgui.hpp&gt;

// Other includes.
#include &lt;iostream&gt;
#include &lt;stdio.h&gt;

using namespace cv;
using namespace std;

int main(int, char**)
{
    // 0. Declaracion de variables (vease documentacion de Mat).
    Mat frame;
    VideoCapture cap;

    // 1. Configuramos camara 0.
    int deviceID = 0;             // 0 = open default camera
    int apiID = cv::CAP_ANY;      // 0 = autodetect default API

    cap.open(deviceID, apiID);

    // 2. Check error.
    if (!cap.isOpened()) {
        cerr &lt;&lt; &quot;ERROR abriendo camara.\n&quot;;
        return -1;
    }

    // 3. Bucle de adquisicion.
    cout &lt;&lt; &quot;Comenzando adquisicion...&quot; &lt;&lt; endl &lt;&lt; &quot;Presiona cualquier tecla para terminar...&quot; &lt;&lt; endl;
    for (;;)
    {
        // 4. Adquisicion de frame (TODO: temporizar la adquisicion y reportar Frames por Segundo (FPS)). 
        cap.read(frame);

        // TODO: Investigar transformaciones en C++ (ver documentacion).

        // 5. Check error.
        if (frame.empty()) {
            cerr &lt;&lt; &quot;ERROR! blank frame grabbed\n&quot;;
            break;
        }

        // 6. Mostramos frame en ventana.
        imshow(&quot;Stream&quot;, frame);
        if (waitKey(5) &gt;= 0)
            break;
    }

    // 7. La camara se liberara en el destructor.
    return 0;
}
</code></pre>

<p>Para compilar y enlazar el anterior código, nos ayudaremos de la herramienta <code>pkg-config</code>, que nos ayudará a
fijar los flags de compilación y enlazado para programas que utilicen OpenCV. Como curiosidad, observa la salida
de la siguiente ejecución:</p>
<pre><code class="sh">pi@raspberrypi:~/Test/Lab1/Camera/CPP $ pkg-config --cflags --libs opencv4
-I/usr/local/include/opencv4 -L/usr/local/lib -lopencv_gapi -lopencv_stitching -lopencv_aruco -lopencv_bgsegm -lopencv_bioinspired -lopencv_ccalib -lopencv_dnn_objdetect -lopencv_dnn_superres -lopencv_dpm -lopencv_highgui -lopencv_face -lopencv_freetype -lopencv_fuzzy -lopencv_hdf -lopencv_hfs -lopencv_img_hash -lopencv_intensity_transform -lopencv_line_descriptor -lopencv_mcc -lopencv_quality -lopencv_rapid -lopencv_reg -lopencv_rgbd -lopencv_saliency -lopencv_stereo -lopencv_structured_light -lopencv_phase_unwrapping -lopencv_superres -lopencv_optflow -lopencv_surface_matching -lopencv_tracking -lopencv_datasets -lopencv_text -lopencv_dnn -lopencv_plot -lopencv_videostab -lopencv_videoio -lopencv_xfeatures2d -lopencv_shape -lopencv_ml -lopencv_ximgproc -lopencv_video -lopencv_xobjdetect -lopencv_objdetect -lopencv_calib3d -lopencv_imgcodecs -lopencv_features2d -lopencv_flann -lopencv_xphoto -lopencv_photo -lopencv_imgproc -lopencv_core
</code></pre>

<p>Estas son las opciones, flags y bibliotecas que será necesario incluir en el proceso de compilación de cualquier prorama
OpenCV. Así, para compilar el anterior programa, puedes utilizar la orden:</p>
<pre><code class="sh">g++ programa.cpp -o programa.x `pkg-config --cflags --libs opencv4`
</code></pre>

<div class="admonition danger">
<p class="admonition-title">Tarea</p>
<p>Compila y ejecuta el anterior programa (está incluido en el paquete proporcionado). Estudia el código y modifícalo
para introducir nuevas transformaciones en las imágenes capturadas (tranformación a otros espacios de color, redimensionado 
de imágenes, etc). Para ello, deberás consultar la documentación de OpenCV.</p>
</div>
<div class="admonition danger">
<p class="admonition-title">Tarea</p>
<p>Temporiza el tiempo de adquisición (<code>cap.read(frame)</code>) y repórtalo a través de línea de comandos, mostrando no sólo el tiempo,
sino los fotogramas por segundo (FPS) obtenidos. Tanto en el caso de C++ como de Python, experimenta con distintas resoluciones
de captura. ¿Cuál es la resolución máxima soportada por la cámara que os proporcionamos?</p>
</div>
<h3 id="reproduccion-de-un-video-desde-fichero">Reproducción de un vídeo desde fichero</h3>
<p>Para reproducir un vídeo almacenado en un fichero, el procedimiento a seguir es similar a la captura desde cámara, 
pero reemplazando el índice de cámara por un nombre de fichero. Utilizaremos la función <code>cv.waitKey()</code> con un valor
apropiado: si es demasiado corto, el vídeo se reproducirá demasiado rápidamente; 
si es demasiado largo, el vídeo se reproducirá demasiado lentamente:</p>
<pre><code class="python">import numpy as np
import cv2 as cv

cap = cv.VideoCapture('vtest.avi')

while cap.isOpened():
    ret, frame = cap.read()

    if not ret:
        print(&quot;Error, saliendo...&quot;)
        break

    gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)

    cv.imshow('frame', gray)

    if cv.waitKey(1) == ord('q'):
        break

cap.release()
cv.destroyAllWindows()
</code></pre>

<h3 id="almacenamiento-de-un-video-en-disco">Almacenamiento de un vídeo en disco</h3>
<p>Para almacenar un <em>frame</em> capturado desde cámara en disco, utilizaremos <code>cv.imwrite()</code> (busca y estudia su 
documentación, y modifica los códigos anteriores para realizar una "fotografía" desde la cámara).</p>
<p>Para almacenar un vídeo, es necesario algo de trabajo adicional. En primera lugar, crearemos un objeto de tipo
<code>VideoWriter</code>, especificando el nombre del fichero de salida deseado. A continuación, especificaremos el llamado
código <em>FourCC</em>, resolución deseada y frames por segundo deseados. El último de los argumentos de la invocación
es el flag <code>isColor</code>, que nos permitirá determinar si el codificador de vídeo espera un frame en color o en escala
de grises (véase la documentación de la función para más información). </p>
<p><a href="http://en.wikipedia.org/wiki/FourCC">FourCC</a> es un código de 4 bytes que permite especificar el codec de vídeo
deseado. Puedes obtener la lista de todos los codigos en <a href="fourcc.org">fourcc.org</a>. Así, por ejemplo, para utilizar
XVID, usaríamos:</p>
<pre><code class="python">fourcc = cv.VideoWriter_fourcc(*'XVID')
</code></pre>

<p>El siguiente código captura desde la cámara, rota cada frame en la dirección vertical, y almacena el vídeo resultante:</p>
<pre><code class="python">import numpy as np
import cv2 as cv

cap = cv.VideoCapture(0)

# Definimos el codec y creamos el objeto VideoWriter.
fourcc = cv.VideoWriter_fourcc(*'XVID')
out = cv.VideoWriter('output.avi', fourcc, 20.0, (640,  480))

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        print(&quot;Error, saliendo...&quot;)
        break
    frame = cv.flip(frame, 0)

    # Escribimos el frame rotado.
    out.write(frame)
    cv.imshow('frame', frame)
    if cv.waitKey(1) == ord('q'):
        break

cap.release()
out.release()

cv.destroyAllWindows()
</code></pre>

<h2 id="tensorflow-lite">TensorFlow Lite</h2>
<p>Una vez estudiados de forma básica los códigos que nos permiten realizar capturas e interacción desde la cámara 
proporcionada, veremos cómo aplicar un modelo preentrenado al mismo, que nos permitirá realizar un proceso de 
<strong>clasificación</strong> de los objetos mostrados en el flujo de vídeo. Nótese que el objetivo de esta práctica no es
estudiar en profundidad el proceso en sí de clasificación, sino simplemente servir como una primera toma de 
contacto con la biblioteca <a href="tensorflow.org/lite">TensorFlow Lite</a>. TFLite es un conjunto de herramientas que 
perimtan ejecutar modelos entrenados TensorFlow en dispostivos móviles, empotrados y en entornos IoT. A diferencia
de Tensorflow, TFLite permite realizar procesos de <strong>inferencia</strong> con tiempos de latencia muy reducidos, y un
<em>footprint</em> también muy reducido. </p>
<p>TFLite consta de dos componentes principales:</p>
<ul>
<li>
<p>El <a href="https://www.tensorflow.org/lite/guide/inference">intérprete de TFLite</a>, que ejecuta modelos especialmente
optimizados en distintos tipos de <em>hardware</em>, incluyendo teléfonos móviles, dispositivos Linux empotrados (e.g. Raspberry Pi) y
microcontroladores.</p>
</li>
<li>
<p>El <a href="https://www.tensorflow.org/lite/convert/index">conversor de TFLite</a>, que convierte modelos TensorFlow para su posterior uso por parte del intérprete, y que puede introducir optimizaciones para reducir el tamaño del modelo y aumentar el rendimiento.</p>
</li>
</ul>
<p>En este primer laboratorio no incidiremos ni en la creación de modelos ni en su conversión a formato <code>tflite</code> propio del 
<em>framework</em> (veremos estas fases en futuros laboratorios); así, partiremos de modelos ya entrenados y convertidos, ya que
el único objetivo en este punto es la familiarización con el entorno.</p>
<p>Las principales características de interés de TFLite son:</p>
<ul>
<li>
<p>Un <a href="https://www.tensorflow.org/lite/guide/inference">intérprete</a> especialmente optimizado para tareas de 
<em>Machine Learning</em> en dispositivos de bajo rendimiento, con soporte para un amplio subconjunto de operadores disponibles
en TensorFlow optimizados para aplicaciones ejecutadas en dicho tipo de dispositivos, enfocados a una reducción del tamaño 
del binario final.</p>
</li>
<li>
<p>Soporte para múltiples plataformas, desde dispositivos Android a IOS, pasando por Linux sobre dispositivos empotrados, o
microcontroladores.</p>
</li>
<li>
<p>APIs para múltiples lenguajes, incluyendo Java, Swift, Objective-C, <strong>C++</strong> y <strong>Python</strong> (estos dos últimos serán de nuestro especial interés).</p>
</li>
<li>
<p>Alto rendimiento, con soporte para <strong>aceleración hardware</strong> sobre dispositivos aceleradores (en nuestro caso, sobre Google Coral) y kernels optmizados para cada tipo de dispositivo.</p>
</li>
<li>
<p>Herramientas de optimización de modelos, que incluyen <a href="https://www.tensorflow.org/lite/performance/post_training_quantization">cuantización</a>, técnica qu estudiaremos en futuros laboratorios, imprescindible para integrar el uso de aceleradores como la Google Coral.</p>
</li>
<li>
<p>Un formato de almacenamiento eficiente, utilizando <a href="https://www.tensorflow.org/lite/convert/index">FlatBuffer</a> optimizado para una reducción de tamaño y en aras de la portabilidad entre dispositivos</p>
</li>
<li>
<p>Un conjunto amplio de <a href="https://www.tensorflow.org/lite/models">modelos preentrenados</a> disponibles directamente para su uso en inferencia.</p>
</li>
</ul>
<p>El flujo básico de trabajo cuando estamos desarrollando una aplicación basada en TFLite se basa en cuatro modelos principales:</p>
<ol>
<li>
<p>Selección de modelo preentrenado o creación/entrenamiento sobre un nuevo modelo. Típicamente utilizando frameworks existentes, como TensorFlow.</p>
</li>
<li>
<p>Conversión del modelo, utilizando el conversor de TFLite desde Python para adaptarlo a las especificidades de TFLite.</p>
</li>
<li>
<p>Despliegue en el dispositivo, utilizando las APIs del lenguaje seleccionado, e invocando al intérprete de TFLite.</p>
</li>
<li>
<p>Optimización del modelo (si es necesaria), utilizando el <a href="https://www.tensorflow.org/lite/performance/model_optimization">Toolkit de Optimización de Modelos</a>, para reducir el tamaño del modelo e incrementar su eficiencia (típicamente a cambio de cierta pérdida en precisión).</p>
</li>
</ol>
<h2 id="clasificacion-basica-de-imagenes-usando-tflite">Clasificación básica de imágenes usando TFLite</h2>
<p>En esta parte del laboratorio, mostraremos el flujo de trabajo básico para aplicar un modelo de clasificación (basado
en la red neuronal Mobilenet), que interactúe con imágenes tomadas directamente desde la cámara web integrada en la Raspberry
Pi. </p>
<div class="admonition danger">
<p class="admonition-title">Tarea</p>
<p>Los ficheros que estudiaremos en esta parte están disponibles en el directorio <code>Clasificacion</code> del paquete proporcionado.</p>
</div>
<p>Como hemos dicho, el objetivo del laboratorio es aplicar inferencia sobre un modelo ya preentrenado, por lo que no incidiremos
en la estructura interna del mismo. Sin embargo, es conveniente saber que <a href="https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md">Mobilenet</a> es una familia de redes neuronales de convolución diseñadas para ser pequeñas en tamaño,
y de baja latencia en inferencia, aplicables a procesos de clasificación, detección o segementación de imágenes, entre otras
muchas aplicaciones. En nuestro caso, la red <code>Mobilenet v1 1.0_224</code> es una red de convolución que acepta imágenes
de dimensión <code>224 x 224</code> y tres canales (RGB), entrenada para devolver la probabilidad de pertenencia a cada una de las 1001
clases para la que ha sido preentrenada.</p>
<p>Antes de comenzar, descarga el modelo, fichero de etiquetas y demás requisitos invocando al script <code>download.sh</code> proporcionado:</p>
<pre><code class="sh">bash download.sh Modelos
</code></pre>

<p>Esta ejecución, si todo ha ido bien, descargará en el directorio <code>Modelos</code> tres ficheros que utilizaremos en el resto del laboratorio:</p>
<ul>
<li><code>mobilenet_v1_1.0_224_quant.tflite</code>: modelo preentrenado y cuantizado MobileNet.</li>
<li><code>mobilenet_v1_1.0_224_quant_edgetpu.tflite</code>: modelo preentrenado y cuantizado MobileNet, compilado con soporte para Google Coral.</li>
<li><code>labels_mobilenet_quant_v1_224.txt</code>: fichero de descripción de etiquetas (clases), con el nombre de una clase por línea. La posición de estas líneas coincide con cada una de las (1001) posiciones del tensor de salida. </li>
</ul>
<h3 id="desarrollo-utilizando-python">Desarrollo utilizando Python</h3>
<p>El fichero <code>classify_opencv.py</code> contiene el código necesario para realizar inferencia (clasificación) de imágenes partiendo de 
capturas de fotogramas desde la cámara de la Raspberry Pi, que revisamos paso a paso a continuación:</p>
<h4 id="invocacion-y-argumentos">Invocación y argumentos</h4>
<p>Observa el inicio de la función <code>main</code> proporcionada:</p>
<pre><code class="python">def main():
  parser = argparse.ArgumentParser(
      formatter_class=argparse.ArgumentDefaultsHelpFormatter)
  parser.add_argument(
      '--model', help='File path of .tflite file.', required=True)
  parser.add_argument(
      '--labels', help='File path of labels file.', required=True)
  args = parser.parse_args()
</code></pre>

<p>El programa recibirá, de forma obligatoria, dos argumentos:</p>
<ul>
<li>El modelo a aplicar, en formato <code>tflite</code> (FlatBuffer), a través del parámetro <code>--model</code>.</li>
<li>El fichero de etiquetas, en formato texto con una etiqueta por línea. Este fichero no es estrictamente obligatorio, pero nos permite mostrar no sólo el número de clase inferida, sino también su texto asociado.</li>
</ul>
<p>Así, podremos ejecutar el programa directamente utilizando la orden (suponiendo que ambos ficheros residen en el directorio
<code>../Modelos</code>):</p>
<pre><code class="sh">python3 classify_opencv.py  --model ../Modelos/mobilenet_v1_1.0_224_quant_edgetpu.tflite   --labels ../Modelos/labels_mobilenet_quant_v1_224.txt
</code></pre>

<p>De momento, no es necesario que ejecutes el código, ya que en primer lugar habrá que realizar algunos ajustes en el código.</p>
<p>La función <code>load_labels</code> simplemente lee el fichero de etiqueta y las almacena en memoria para su posterior procesamiento tras la inferencia.</p>
<h4 id="preparacion-del-interprete-tflite">Preparación del intérprete TFLite</h4>
<p>El siguiente paso es la preparación del intérprete de TFLite:</p>
<pre><code class="python">  interpreter = Interpreter(args.model)
</code></pre>

<p>Observa que el único parámetro proporcionado es el nombre del modelo a cargar en formato TFLite. 
Observa también que necesitaremos cargar los módulos correspondientes a TFLite antes de hacer uso de esta función:</p>
<pre><code class="python">from tflite_runtime.interpreter import Interpreter
</code></pre>

<p>A continuación, obtenemos información sobre el tensor de entrada del modelo recién cargado, utilizando la función 
<code>get_input_details</code>, y consultando la propiedad <code>shape</code> de dicha entrada. Esto nos devolverá en las variables 
<code>widght</code> y <code>height</code> los tamaños de imagen esperados por el modelo. A partir de ahora, puedes consultar la forma de
trabajar con la API de Python a través de la <a href="https://www.tensorflow.org/lite/api_docs/python/tf/lite">documentación oficial</a>.</p>
<p>Utilizaremos esta información para redimensionar la imagen capturada de la cámara como paso previo a la invocación del modelo.</p>
<div class="admonition danger">
<p class="admonition-title">Tarea</p>
<p>Asegúratge de que ambas líneas (creación del intérprete e importación de bibliotecas) son correctas. Comprueba el correcto funcionamiento del código utilizando el modelo <code>mobilenet_v1_1.0_224_quant.tflite</code> como entrada al programa <code>classify_opencv.py</code> sobre tu Raspberry Pi. Si todo ha ido bien, deberías ver una ventana mostrando la salida de la cámara con cierta información sobreimpresionada, y para cada fotograma, el resultado de la inferencia a través de línea de comandos.</p>
</div>
<h4 id="inferencia">Inferencia</h4>
<p>En el bucle principal de captura, se invoca a la función <code>classify_image</code>. Esta es una función propia, que
recibe simplemente el intérprete TFLite construido y la imagen capturada, pero cuyo cuerpo
contiene cierta funcionalidad de interés:</p>
<pre><code class="python">## Invoke model and process output (quantization-aware).
def classify_image(interpreter, image, top_k=1):
  &quot;&quot;&quot;Returns a sorted array of classification results.&quot;&quot;&quot;
  set_input_tensor(interpreter, image)

  interpreter.invoke()

  output_details = interpreter.get_output_details()[0]
  output = np.squeeze(interpreter.get_tensor(output_details['index']))

  # If the model is quantized (uint8 data), then dequantize the results
  if output_details['dtype'] == np.uint8:
    scale, zero_point = output_details['quantization']
    output = scale * (output - zero_point)

  ordered = np.argpartition(-output, top_k)
  return [(i, output[i]) for i in ordered[:top_k]]
</code></pre>

<p>Observa que la función opera en varias fases. En primer lugar, se obtiene una referencia al tensor de entrada
del modelo (invocando a la función propia <code>set_input_tensor</code>). En este caso, se copia, elemento a elemento,
la imagen de entrada (<code>image</code>) a dicho tensor:</p>
<pre><code class="python">## Establish input tensor from an image (copying).
def set_input_tensor(interpreter, image):
  tensor_index = interpreter.get_input_details()[0]['index']
  input_tensor = interpreter.tensor(tensor_index)()[0]
  input_tensor[:, :] = image
</code></pre>

<p>Volviendo a la función <code>classify_image</code>, una vez copiada la entrada al tensor de entrada del modelo, se invoca
al modelo TFLite (<code>interpreter.invoke()</code>). <strong>Este es el proceso de inferencia o aplicación del modelo</strong>, y su tiempo
de respuesta es crítico.</p>
<p>Por último, se procesa la salida (tensor de salida). En caso de ser una salida cuantizada (esto es, el tipo
de cada elemento del array de salida es <em>uint8_t</em>, veremos más sobre cuantización
en futuros laboratorios), ésta debe procesarse de forma acorde a los parámetros de cuantización 
utilizados.</p>
<p>Al final, la función devolverá un array de tuplas con la posición/clase (<code>i</code>) y la probabilidad de pertenencia del objeto
observado a dicha clase (<code>output[i]</code>).</p>
<div class="admonition danger">
<p class="admonition-title">Tarea</p>
<p>Imprime por pantalla la información sobre el tensor de entrada y el tensor de salida y analiza la salida proporcionada.</p>
</div>
<p>Obsérvese que, de forma previa a la invocación del modelo, la imágen capturada se ha reescalado de forma acorde al tamaño
del tensor de entrada del modelo:</p>
<pre><code class="python">    image = cv2.resize(frame, (224, 224), interpolation = cv2.INTER_AREA)
</code></pre>

<h4 id="postprocesamiento">Postprocesamiento</h4>
<p>Por último, el programa sobreimpresiona información sobre etiqueta de clasificación, probabilidad de pertenencia a la clase y 
tiempo de inferencia sobre el propio <em>frame</em>, mostrando la imagen resultante:</p>
<pre><code class="python">   cv2.putText(frame, '%s %.2f\n%.1fms' % ( labels[label_id], prob, elapsed_ms ), bottomLeftCornerOfText, font, fontScale, fontColor, lineType)
   cv2.imshow('image',frame)
</code></pre>

<h3 id="uso-de-google-coral">Uso de Google Coral</h3>
<p>Para utilizar el acelerador Google Coral (que debe estar conectado a la Raspberry Pi), realizaremos ciertas modificaciones en 
el código, que en este caso son mínimas. </p>
<p>En primer lugar, añadiremos un <code>import</code> en nuestro fichero:</p>
<pre><code class="python">from tflite_runtime.interpreter import load_delegate
</code></pre>

<p>A continuación, reemplazaremos la construcción del intérprete:</p>
<pre><code class="python">interpreter = Interpreter(args.model)
</code></pre>

<p>Por la especificación de una biblioteca delegada para realizar la inferencia sobre la Google Coral:</p>
<pre><code class="python">interpreter = Interpreter(args.model,
    experimental_delegates=[load_delegate('libedgetpu.so.1.0')])
</code></pre>

<p>El fichero <code>libedgetpu.so.1.0</code> viene ya instalado en la imagen proporcionada, aunque es directamente instalable siguiendo las instrucciones de instalación proporcionadas por el fabricante.</p>
<p>Finalmente, será necesario aplicar un modelo especialmente compilado para la Google Coral. Normalmente, este modelo se obtiene 
utilizando el <a href="https://coral.withgoogle.com/docs/edgetpu/compiler/">compilador de la Edge TPU</a>, pero en este caso se descarga y proporciona mediante el script <code>download.sh</code>. El nombre del modelo es <code>mobilenet_v1_1.0_224_quant_edgetpu.tflite</code>.</p>
<p>Así, podremos ejecutar sobre la Edge TPU usando:</p>
<pre><code class="sh">python3 classify_opencv.py \
  --model ../Modelos/mobilenet_v1_1.0_224_quant_edgetpu.tflite \
  --labels ../Modelos/labels_mobilenet_quant_v1_224.txt
</code></pre>

<p>Obviamente, tu Google Coral deberá estar conectada a la Raspberry Pi para que el anterior comando tenga éxito.</p>
<div class="admonition danger">
<p class="admonition-title">Tarea</p>
<p>Compara los tiempos de ejecución de la inferencia utilizando el procesdor de propósito general frente al rendimiento utilizando la Google Coral. ¿Qué ganancia de rendimiento observas?</p>
</div>
<h3 id="desarrollo-utilizando-c">Desarrollo utilizando C++</h3>
<p>El rendimiento es un factor determinante en aplicaciones <em>Edge computing</em>, por lo que resultará interesante disponer de una 
base desarrollada en C++ sobre la que trabajar para el ejemplo de clasificación.</p>
<p>El fichero <code>classification.cpp</code> proporciona un flujo de trabajo completo para realizar una clasificación de imágenes similar a la realizada anteriormente usando la API de Python. </p>
<p>En primer lugar, compila y ejecuta el programa para validar su funcionamiento:</p>
<pre><code class="sh">g++ classification.cpp -I /home/pi/tensorflow/ /home/pi/tensorflow/tensorflow/lite/tools/make/gen/linux_aarch64/lib/libtensorflow-lite.a -lpthread -ldl `pkg-config --cflags --libs opencv4` -o classification.x

./classification.x
</code></pre>

<p>Observa que se utiliza la biblioteca <code>libtensorflow-lite.a</code>, disponible en cualquier instalación TensorFlow.</p>
<p>La salida está preparada para mostrar el código numérico de la clase detectada con mayor probabilidad, dicha probabilidad, y 
la descripción textual de la clase.</p>
<p>El código desarrollado es similar, paso a paso, al descrito para Python, por lo que no se incidirá en los detalles más allá
de la <a href="https://www.tensorflow.org/lite/api_docs/cc">API utilizada</a>:</p>
<h4 id="ficheros-de-cabecera">Ficheros de cabecera</h4>
<p>Incluiremos ficheros de cabecera genéricos, para OpenCV y para TFLite:</p>
<pre><code class="cpp">#include &lt;stdio.h&gt;

// TFLite includes.
#include &quot;tensorflow/lite/interpreter.h&quot;
#include &quot;tensorflow/lite/kernels/register.h&quot;
#include &quot;tensorflow/lite/model.h&quot;
#include &quot;tensorflow/lite/tools/gen_op_registration.h&quot;

// OpenCV includes.
#include &lt;opencv2/core.hpp&gt;
#include &lt;opencv2/videoio.hpp&gt;
#include &lt;opencv2/imgproc.hpp&gt;
#include &lt;opencv2/highgui.hpp&gt;
#include &lt;iostream&gt;
#include &lt;fstream&gt;
#include &lt;stdio.h&gt;

// C++ sorting utils.
#include &lt;vector&gt;
#include &lt;numeric&gt;      // std::iota
#include &lt;algorithm&gt;    // std::sort, std::stable_sort
</code></pre>

<h4 id="carga-del-modelo-desde-un-fichero">Carga del modelo desde un fichero</h4>
<p>En este ejemplo, se realiza la carga del modelo directamente desde un fichero en disco, utilizando la 
rutina <code>BuildFromFile</code>:</p>
<pre><code class="cpp">   // 1. Cargamos modelo desde un fichero.
   std::unique_ptr&lt;tflite::FlatBufferModel&gt; model = tflite::FlatBufferModel::BuildFromFile(&quot;../Modelos/mobilenet_v1_1.0_224_quant.tflite&quot;);

   if(!model){
        printf(&quot;Failed to mmap model\n&quot;);
        exit(0);
   }

   // 2. Construimos el iterprete TFLite.
   tflite::ops::builtin::BuiltinOpResolver resolver;
   std::unique_ptr&lt;tflite::Interpreter&gt; interpreter;
   tflite::InterpreterBuilder(*model.get(), resolver)(&amp;interpreter);

   // 3. Alojamos espacio para tensores.
   interpreter-&gt;AllocateTensors();
</code></pre>

<p>Observa que, en el anterior fragmento de código, además de la carga del modelo, se construye un intérprete utilizando
la clase <code>InterpreterBuilder</code>, y se aloja espacio para los tensores necesarios para aplicar el modelo.</p>
<h4 id="caracaterizacion-de-tensores-de-entrada-y-salida">Caracaterización de tensores de entrada y salida</h4>
<p>Como hemos hecho en el código Python, será necesario realizar una caracterización de los tensores de entrada y salida. El primero,
para copiar nuestra imagen capturada desde cámara; el segundo, para procesar la salida obtenida:</p>
<pre><code class="cpp">    // 4. Identificamos el tensor de entrada y de salida.
    int input_number  = interpreter-&gt;inputs()[0];
    uint8_t * input_tensor  = interpreter-&gt;typed_tensor&lt;uint8_t&gt;(input_number);

    int output_number = interpreter-&gt;outputs()[0];
    uint8_t * output_tensor = interpreter-&gt;typed_tensor&lt;uint8_t&gt;(output_number);
</code></pre>

<div class="admonition danger">
<p class="admonition-title">Tarea</p>
<p>¿Cuál es el índice de los tensores de entrada y salida generados? Coinciden con los observados al mostrar por pantalla la información sobre ellos en el código Python.</p>
</div>
<h4 id="invocacion-del-modelo-tflite">Invocación del modelo TFLite</h4>
<p>Tras comenzar con la captura de vídeo y redimensionar la imagen de entrada, copiaremos al tensor de entrada
la imagen capturada, pixel a pixel:</p>
<pre><code class="cpp">   // 8. Copiamos imagen al tensor de entrada.
   for (int i = 0; i &lt; 224*224; ++i) {
      input_tensor[3*i + 0] = frame.at&lt;cv::Vec3b&gt;(i)[0];
      input_tensor[3*i + 1] = frame.at&lt;cv::Vec3b&gt;(i)[1];
      input_tensor[3*i + 2] = frame.at&lt;cv::Vec3b&gt;(i)[2];
   }
</code></pre>

<p>A continuación, invocamos al modelo:</p>
<pre><code class="cpp">   // 9. Invocamos al modelo.
   if (interpreter-&gt;Invoke() != kTfLiteOk) {
     cerr &lt;&lt; &quot;Failed to invoke tflite!&quot;;
     exit(-1);
   }
</code></pre>

<h4 id="clasificacion-y-analisis-de-salida">Clasificación y análisis de salida</h4>
<p>El proceso de análisis de salida es ligeramente distinto al usado en Python, aunque sigue una filosofía similar. En primer lugar, analizamos el tensor de salida:</p>
<pre><code class="cpp">   // 10. Analizamos el tamaño del tensor de salida.
   TfLiteIntArray* output_dims = interpreter-&gt;tensor(output_number)-&gt;dims;
   auto output_size = output_dims-&gt;data[output_dims-&gt;size - 1];
   cout &lt;&lt; output_size &lt;&lt; endl;
</code></pre>

<p>Como en el caso de Python, en función de la cuantización de la salida, deberemos procesarla de forma acorde (trataremos la cuantización en futuros laboratorios). 
En cualquier caso, el array <code>logits</code> contiene la probabilidad de pertenencia a cada una de las 1001 clases disponibles. El código que se os proporciona ordena dicho array y muestra por pantalla la clase más probable, junto a su probabilidad asociada.</p>
<div class="admonition danger">
<p class="admonition-title">Tarea</p>
<p>Temporiza, utilizando la rutinas de la clase <code>chrono</code> de C++, el proceso de inferencia, y compáralo con el de la versión Python.</p>
</div>
<div class="admonition danger">
<p class="admonition-title">Tarea</p>
<p>De forma opcional, investiga cómo sobreimpresionar la información asociada al proceso de inferencia (clase, probabilidad y tiempo) de forma similar a cómo lo hicimos en Python.</p>
</div>
<p>En futuros laboratorios veremos también cómo utilizar el acelerador Google Coral desde código C++.</p>
<h2 id="deteccion-de-objetos-usando-tflite">Detección de objetos usando TFLite</h2>
<p>Como segundo caso de uso, proporcionamos un programa completo para la detección de objetos utilizando una red neuronal de convolución ya entrenada (MobileNet SSD). El ejemplo proporcionado, que utiliza la API de Python, es equivalente en estructura al observado para clasificación de imágenes, pero en este caso, la red neuronal nos proporciona cuatro tensores de salida en lugar de uno, como puedes observar en la función <code>detect_objects</code>:</p>
<pre><code class="python">def detect_objects(interpreter, image, threshold):
  &quot;&quot;&quot;Returns a list of detection results, each a dictionary of object info.&quot;&quot;&quot;
  set_input_tensor(interpreter, image)
  interpreter.invoke()

  # Get all output details
  boxes = get_output_tensor(interpreter, 0)
  classes = get_output_tensor(interpreter, 1)
  scores = get_output_tensor(interpreter, 2)
  count = int(get_output_tensor(interpreter, 3))

  results = []
  for i in range(count):
    if scores[i] &gt;= threshold:
      result = {
          'bounding_box': boxes[i],
          'class_id': classes[i],
          'score': scores[i]
      }
      results.append(result)
  return results
</code></pre>

<p>Observa que, tras la invocación del modelo, se obtienen cuatro tensores de salida, que respectivamente
almacenan las <em>cajas</em> o <em>bounding boxes</em> de los objetos detectados, la identificación de la clase a la que
pertenecen, el <em>score</em> de pertenencia a dichas clases y el número de objetos detectado, mostrándose dicha información
por pantalla.</p>
<p>La función <code>annotate_bojects</code> superpone la <em>bounding box</em> en la imagen capturada, mostrando el resultado.</p>
<p>El resto de funcionalidad del ejemplo es exactamente igual al descrito para la clasificación de imágenes.</p>
<div class="admonition danger">
<p class="admonition-title">Tarea</p>
<p>Comprueba el funcionamiento del ejemplo proporcionado, haciendo ver múltiples objetos cotidianos a través de la cámara. Juega en el parámetro <code>--threshold</code>, que indica el umbral de <em>score</em> mínimo para considerar la detección de un objeto.</p>
</div>
<div class="admonition danger">
<p class="admonition-title">Tarea</p>
<p>Transforma el código para utilizar el acelerador Google Coral y realiza una comparativa de tiempos de inferencia.</p>
</div>
<div class="admonition danger">
<p class="admonition-title">Tarea (opcional)</p>
<p>Obtén una implementación en C++ del código de detección de objetos. En principio, esta implementación no será necesaria hasta el laboratorio correspondiente a detección de objetos, pero resulta conveniente adelantar esta tarea a este laboratorio inicial.</p>
</div></div>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../js/base.js" defer></script>
        <script src="../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="Search Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Search</h4>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form role="form">
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="Keyboard Shortcuts Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Keyboard Shortcuts</h4>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
