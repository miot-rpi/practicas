<!DOCTYPE html>
<html lang="en" data-bs-theme="light">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../img/favicon.ico">
        <title>Práctica 1 - Master IoT UCM - Prácticas RPI/ANIOT/LSI (25/26)</title>
        <link href="../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../css/fontawesome.min.css" rel="stylesheet">
        <link href="../../css/brands.min.css" rel="stylesheet">
        <link href="../../css/solid.min.css" rel="stylesheet">
        <link href="../../css/v4-font-face.min.css" rel="stylesheet">
        <link href="../../css/base.css" rel="stylesheet">
        <link id="hljs-light" rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" >
        <link id="hljs-dark" rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github-dark.min.css" disabled>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
        <script>hljs.highlightAll();</script> 
    </head>

    <body>
        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">
                <a class="navbar-brand" href="../..">Master IoT UCM - Prácticas RPI/ANIOT/LSI (25/26)</a>
                <!-- Expander button -->
                <button type="button" class="navbar-toggler" data-bs-toggle="collapse" data-bs-target="#navbar-collapse" aria-controls="navbar-collapse" aria-expanded="false" aria-label="Toggle navigation">
                    <span class="navbar-toggler-icon"></span>
                </button>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="nav-item">
                                <a href="../.." class="nav-link">Calendario</a>
                            </li>
                            <li class="nav-item dropdown">
                                <a href="#" class="nav-link dropdown-toggle" role="button" data-bs-toggle="dropdown"  aria-expanded="false">RPI-I</a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../../RPI-I/P1/" class="dropdown-item">LAB1. Introducción al entorno de desarrollo ESP-IDF</a>
</li>
                                    
<li>
    <a href="../../RPI-I/P2/" class="dropdown-item">LAB2. WiFi en el ESP32</a>
</li>
                                    
<li>
    <a href="../../RPI-I/P3/" class="dropdown-item">LAB3. WiFi: provisionamiento y ahorro de energía</a>
</li>
                                    
<li>
    <a href="../../RPI-I/P4/" class="dropdown-item">LAB4. ESP WiFi Mesh</a>
</li>
                                    
<li>
    <a href="../../RPI-I/P5/" class="dropdown-item">LAB5. BLE: servidor GATT</a>
</li>
                                    
<li>
    <a href="../../RPI-I/P6/" class="dropdown-item">LAB6. BLE: cliente GATT</a>
</li>
                                    
<li>
    <a href="../../RPI-I/P7/" class="dropdown-item">LAB7. BLE Mesh</a>
</li>
                                    
<li>
    <a href="../../RPI-I/P8/" class="dropdown-item">LAB8. 802.15.4 y Thread</a>
</li>
                                    
<li>
    <a href="../../RPI-I/P9/" class="dropdown-item">LAB9. 6LoWPAN y simulador Cooja</a>
</li>
                                    
<li>
    <a href="../../RPI-I/P10/" class="dropdown-item">LAB10. LoRa y LoRaWan</a>
</li>
                                </ul>
                            </li>
                            <li class="nav-item dropdown">
                                <a href="#" class="nav-link dropdown-toggle" role="button" data-bs-toggle="dropdown"  aria-expanded="false">RPI-II</a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../../RPI-II/P1_I/" class="dropdown-item">Práctica 1 (I)</a>
</li>
                                    
<li>
    <a href="../../RPI-II/P1_III/" class="dropdown-item">Práctica 1 (II)</a>
</li>
                                    
<li>
    <a href="../../RPI-II/P2/" class="dropdown-item">Práctica 2</a>
</li>
                                    
<li>
    <a href="../../RPI-II/P3/" class="dropdown-item">Práctica 3</a>
</li>
                                    
<li>
    <a href="../../RPI-II/P4/" class="dropdown-item">Práctica 4</a>
</li>
                                    
<li>
    <a href="../../RPI-II/P5/" class="dropdown-item">Práctica 5 (I)</a>
</li>
                                    
<li>
    <a href="../../RPI-II/P5_II/index.md" class="dropdown-item">Práctica 5 (II)</a>
</li>
                                    
<li>
    <a href="../../RPI-II/P6/" class="dropdown-item">Práctica 6</a>
</li>
                                    
<li>
    <a href="../../RPI-II/P7/index.md" class="dropdown-item">Práctica 7</a>
</li>
                                    
<li>
    <a href="../../RPI-II/P8/index.md" class="dropdown-item">Práctica 8</a>
</li>
                                </ul>
                            </li>
                            <li class="nav-item dropdown">
                                <a href="#" class="nav-link dropdown-toggle" role="button" data-bs-toggle="dropdown"  aria-expanded="false">ANIOT</a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../../ANIOT/P1/" class="dropdown-item">Práctica 1</a>
</li>
                                    
<li>
    <a href="../../ANIOT/P1b/" class="dropdown-item">Práctica 1b</a>
</li>
                                    
<li>
    <a href="../../ANIOT/P2/" class="dropdown-item">Práctica 2</a>
</li>
                                    
<li>
    <a href="../../ANIOT/P3/" class="dropdown-item">Práctica 3</a>
</li>
                                    
<li>
    <a href="../../ANIOT/P4/" class="dropdown-item">Práctica 4</a>
</li>
                                    
<li>
    <a href="../../ANIOT/P5/" class="dropdown-item">Práctica 5</a>
</li>
                                    
<li>
    <a href="../../ANIOT/P6/" class="dropdown-item">Práctica 6</a>
</li>
                                    
<li>
    <a href="../../ANIOT/P7/" class="dropdown-item">Práctica 7</a>
</li>
                                    
<li>
    <a href="../../ANIOT/P7/index2/" class="dropdown-item">Práctica 7 (adicional)</a>
</li>
                                    
<li>
    <a href="../../ANIOT/P8/" class="dropdown-item">Práctica 8</a>
</li>
                                </ul>
                            </li>
                            <li class="nav-item dropdown">
                                <a href="#" class="nav-link dropdown-toggle active" aria-current="page" role="button" data-bs-toggle="dropdown"  aria-expanded="false">LSI</a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../Lab0/" class="dropdown-item">Práctica 0</a>
</li>
                                    
<li>
    <a href="./" class="dropdown-item active" aria-current="page">Práctica 1</a>
</li>
                                    
<li>
    <a href="../Lab2/" class="dropdown-item">Práctica 2</a>
</li>
                                    
<li>
    <a href="../Lab3/" class="dropdown-item">Práctica 3</a>
</li>
                                    
<li>
    <a href="../Lab4/" class="dropdown-item">Práctica 4</a>
</li>
                                    
<li>
    <a href="../Lab5/" class="dropdown-item">Práctica 5</a>
</li>
                                    
<li>
    <a href="../Lab6/" class="dropdown-item">Práctica 6</a>
</li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ms-md-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-bs-toggle="modal" data-bs-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../Lab0/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../Lab2/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            <div class="row">
                    <div class="col-md-3"><div class="navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-bs-toggle="collapse" data-bs-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>

    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-body-tertiary">
        <ul class="nav flex-column">
            
            <li class="nav-item" data-bs-level="1"><a href="#laboratorio-1-introduccion-a-tflite-sobre-la-raspberry-pi" class="nav-link">Laboratorio 1. Introducción a TFLite sobre la Raspberry Pi</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-bs-level="2"><a href="#objetivos" class="nav-link">Objetivos</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#1-tensorflow-lite" class="nav-link">1. TensorFlow Lite</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#2-clasificacion-de-imagenes-usando-tflite" class="nav-link">2. Clasificación de imágenes usando TFLite</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#uso-de-google-coral" class="nav-link">Uso de Google Coral</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
        </ul>
    </div>
</div></div>
                    <div class="col-md-9" role="main">

<h1 id="laboratorio-1-introduccion-a-tflite-sobre-la-raspberry-pi">Laboratorio 1. Introducción a TFLite sobre la Raspberry Pi</h1>
<h2 id="objetivos">Objetivos</h2>
<ul>
<li>Familiarizarse con TensorFlow Lite</li>
<li>Desarrollar una aplicación básica de clasificación de imágenes combinando las APIs de OpenCV y TFLite desde Python y desde C++.</li>
<li>Acelerar el proceso de inferencia utilizando el acelerador Google Coral.</li>
</ul>
<p>Puedes obtener los ficheros necesarios para el desarrollo de la práctica <a href="https://drive.google.com/file/d/1LS1snQqbnz0sQRNRmD-F0KzEMfBA2zov/view?usp=sharing">aquí</a> .</p>
<h2 id="1-tensorflow-lite">1. TensorFlow Lite</h2>
<p>Una vez estudiados de forma básica los códigos que nos permiten realizar capturas e interacción desde la cámara, veremos cómo aplicar un modelo pre-entrenado, que nos permitirá realizar un proceso de clasificación de los objetos en el flujo de vídeo. Nótese que el objetivo de esta práctica no es estudiar en profundidad el proceso en sí de clasificación, sino simplemente servir como una primera toma de contacto con la biblioteca <a href="https://ai.google.dev/edge/litert">TensorFlow Lite</a>, recientemente rebautizada como <a href="https://developers.googleblog.com/es/tensorflow-lite-is-now-litert/">LiteRT</a>. </p>
<h3 id="que-es-tflitelitert">¿Qué es TFLite/LiteRT?</h3>
<p>TFLite o LiteRT es un conjunto de herramientas que permiten ejecutar modelos entrenados en <a href="https://www.tensorflow.org/">TensorFlow</a> en dispositivos móviles, empotrados y en entornos IoT. A diferencia de Tensorflow, TFLite permite realizar procesos de <em>inferencia</em> con tiempos de latencia muy reducidos, y un <em>footprint</em> también muy reducido. </p>
<p>TFLite consta de dos componentes principales:</p>
<ul>
<li>
<p>El <a href="https://www.tensorflow.org/lite/guide/inference">intérprete de TFLite</a>, que ejecuta modelos especialmente optimizados en distintos tipos de <em>hardware</em>, incluyendo teléfonos móviles, dispositivos Linux empotrados (e.g. Raspberry Pi) y microcontroladores.</p>
</li>
<li>
<p>El <a href="https://www.tensorflow.org/lite/convert/index">conversor de TFLite</a>, que convierte modelos TensorFlow para su posterior uso por parte del intérprete, y que puede introducir optimizaciones para reducir el tamaño del modelo y aumentar el rendimiento.</p>
</li>
</ul>
<p>En este primer laboratorio no incidiremos ni en la creación de modelos ni en su conversión a formato <code>tflite</code> propio del <em>framework</em> (veremos estas fases en futuros laboratorios); así, partiremos de modelos ya entrenados y convertidos, ya que el único objetivo en este punto es la familiarizarse con el entorno.</p>
<p>Las principales características de interés de TFLite son:</p>
<ul>
<li>
<p>Un <a href="https://www.tensorflow.org/lite/guide/inference">intérprete</a> especialmente optimizado para tareas de <em>Machine Learning</em> en dispositivos de bajo rendimiento, con soporte para un amplio subconjunto de operadores disponibles en TensorFlow optimizados para aplicaciones ejecutadas en dicho tipo de dispositivos, enfocados a una reducción del tamaño del binario final.</p>
</li>
<li>
<p>Soporte para múltiples plataformas, desde dispositivos Android a IOS, pasando por Linux sobre dispositivos empotrados, o microcontroladores.</p>
</li>
<li>
<p>APIs para múltiples lenguajes, incluyendo Java, Swift, Objective-C, C++ y Python (estos dos últimos serán de nuestro especial interés).</p>
</li>
<li>
<p>Alto rendimiento, con soporte para aceleración hardware sobre dispositivos aceleradores (en nuestro caso, sobre Google Coral) y kernels optimizados para cada tipo de dispositivo.</p>
</li>
<li>
<p>Herramientas de optimización de modelos, que incluyen <a href="https://www.tensorflow.org/lite/performance/post_training_quantization">cuantización</a>, técnica qu estudiaremos en futuros laboratorios, imprescindible para integrar el uso de aceleradores como la Google Coral.</p>
</li>
<li>
<p>Un formato de almacenamiento eficiente, utilizando <a href="https://flatbuffers.dev">FlatBuffer</a> optimizado para una reducción de tamaño y en aras de la portabilidad entre dispositivos</p>
</li>
<li>
<p>Un conjunto amplio de <a href="https://www.tensorflow.org/lite/models">modelos pre-entrenados</a> disponibles directamente para su uso en inferencia.</p>
</li>
</ul>
<p>El flujo básico de trabajo cuando estamos desarrollando una aplicación basada en TFLite se basa en cuatro modelos principales:</p>
<ol>
<li>
<p>Selección de modelo pre-entrenado o creación/entrenamiento sobre un nuevo modelo. Típicamente utilizando frameworks existentes, como TensorFlow.</p>
</li>
<li>
<p>Conversión del modelo, utilizando el conversor de TFLite desde Python para adaptarlo a las especificidades de TFLite.</p>
</li>
<li>
<p>Despliegue en el dispositivo, utilizando las APIs del lenguaje seleccionado, e invocando al intérprete de TFLite.</p>
</li>
<li>
<p>Optimización del modelo (si es necesaria), utilizando el <a href="https://www.tensorflow.org/lite/performance/model_optimization">Toolkit de Optimización de Modelos</a>, para reducir el tamaño del modelo e incrementar su eficiencia (típicamente a cambio de cierta pérdida en precisión).</p>
</li>
</ol>
<h3 id="instalacion-tflite">Instalación TFLite</h3>
<p>Lo más sencillo es buscar un binario reciente pre-compilado para Raspberry Pi OS 64 <em>Bookworm</em> como por ejemplo los proporcionados por el repositorio <a href="https://github.com/prepkg/tensorflow-lite-raspberrypi?tab=readme-ov-file">tensorflow-lite-raspberrypi</a>. </p>
<p>El proceso de instalación es sencillo:</p>
<pre><code class="language-sh">$ wget https://github.com/prepkg/tensorflow-lite-raspberrypi/releases/download/2.16.1/tensorflow-lite_64.deb
$ sudo apt install -y ./tensorflow-lite_64.deb
</code></pre>
<h2 id="2-clasificacion-de-imagenes-usando-tflite">2. Clasificación de imágenes usando TFLite</h2>
<p>En esta parte del laboratorio, mostraremos el flujo de trabajo básico para aplicar un modelo de clasificación (basado en la red neuronal <em>Mobilenet</em>), que interactúe con imágenes tomadas directamente desde la cámara web integrada en la Raspberry Pi. </p>
<div class="admonition danger">
<p class="admonition-title">Tarea</p>
<p>Los ficheros que estudiaremos en esta parte están disponibles en el directorio <code>Clasificacion</code> del paquete proporcionado.</p>
</div>
<p>Como hemos dicho, el objetivo del laboratorio es aplicar inferencia sobre un modelo ya pre-entrenado, por lo que no incidiremos en la estructura interna del mismo. Sin embargo, es conveniente saber que <a href="https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md">Mobilenet</a> es una familia de redes neuronales de convolución diseñadas para ser pequeñas en tamaño,
y de baja latencia en inferencia, aplicables a procesos de clasificación, detección o segmentación de imágenes, entre otras muchas aplicaciones. En nuestro caso, la red <code>Mobilenet v1 1.0_224</code> es una red de convolución que acepta imágenes de dimensión <code>224 x 224</code> y tres canales (RGB), entrenada para devolver la probabilidad de pertenencia a cada una de las 1001 clases para la que ha sido pre-entrenada.</p>
<p>Antes de comenzar, es preciso descargar el modelo, fichero de etiquetas y demás requisitos invocando al script <code>download.sh</code> proporcionado:</p>
<pre><code class="language-sh">$ sh download.sh Modelos
</code></pre>
<p>Esta ejecución, si todo ha ido bien, descargará en el directorio <code>Modelos</code> tres ficheros que utilizaremos en el resto del laboratorio:</p>
<ul>
<li><code>mobilenet_v1_1.0_224_quant.tflite</code>: modelo pre-entrenado y cuantizado MobileNet.</li>
<li><code>mobilenet_v1_1.0_224_quant_edgetpu.tflite</code>: modelo pre-entrenado y cuantizado MobileNet, compilado con soporte para Google Coral.</li>
<li><code>labels_mobilenet_quant_v1_224.txt</code>: fichero de descripción de etiquetas (clases), con el nombre de una clase por línea. La posición de estas líneas coincide con cada una de las (1001) posiciones del tensor de salida. </li>
</ul>
<h3 id="desarrollo-utilizando-python">Desarrollo utilizando Python</h3>
<div class="admonition warning">
<p class="admonition-title">Aviso</p>
<p>Debian <em>Bookworm</em> incluye por defecto Python 3.11 que exige la utilización de entornos virtuales para la gestión de módulos. Por lo tanto antes de ejecutar alguno de los comandos siguientes es preciso crear un entorno virtual, por ejemplo con los siguientes comandos: </p>
</div>
<pre><code class="language-sh">$ python3 -m venv LSI_venv --system-site-packages
$ source LSI_venv/bin/activate
</code></pre>
<div class="admonition warning">
<p class="admonition-title">Aviso</p>
<p>Al trabajar con entornos virtuales es preciso instalar los paquetes Python necesarios dentro del entorno virtual mediante ejecutando el siguiente comando desde el directorio donde se encuentre el fichero correspondiente: </p>
</div>
<pre><code class="language-sh">$ python3 -m pip install -r requirements.txt
</code></pre>
<p>El fichero <code>classify_opencv.py</code> contiene el código necesario para realizar inferencia (clasificación) de imágenes partiendo de capturas de fotogramas desde la cámara de la Raspberry Pi, que revisamos paso a paso a continuación:</p>
<h4 id="invocacion-y-argumentos">Invocación y argumentos</h4>
<p>Observa el inicio de la función <code>main</code> proporcionada:</p>
<pre><code class="language-python">def main():
  parser = argparse.ArgumentParser(
      formatter_class=argparse.ArgumentDefaultsHelpFormatter)
  parser.add_argument(
      '--model', help='File path of .tflite file.', required=True)
  parser.add_argument(
      '--labels', help='File path of labels file.', required=True)
  args = parser.parse_args()
</code></pre>
<p>El programa recibirá, de forma obligatoria, dos argumentos:</p>
<ul>
<li>El modelo a aplicar, en formato <code>tflite</code> (FlatBuffer), a través del parámetro <code>--model</code>.</li>
<li>El fichero de etiquetas, en formato texto con una etiqueta por línea. Este fichero no es estrictamente obligatorio, pero nos permite mostrar no sólo el número de clase inferida, sino también su texto asociado.</li>
</ul>
<p>Así, podremos ejecutar el programa directamente utilizando la orden (suponiendo que ambos ficheros residen en el directorio
<code>../Modelos</code>):</p>
<pre><code class="language-sh">python3 classify_opencv.py  --model ../Modelos/mobilenet_v1_1.0_224_quant.tflite   \
    --labels ../Modelos/labels_mobilenet_quant_v1_224.txt
</code></pre>
<p>La función <code>load_labels</code> simplemente lee el fichero de etiqueta y las almacena en memoria para su posterior procesamiento tras la inferencia.</p>
<h4 id="preparacion-del-interprete-tflite">Preparación del intérprete TFLite</h4>
<p>El siguiente paso es la preparación del intérprete de TFLite:</p>
<pre><code class="language-python">interpreter = Interpreter(args.model)
</code></pre>
<p>Observa que el único parámetro proporcionado es el nombre del modelo a cargar en formato TFLite. Observa también que necesitaremos cargar los módulos correspondientes a TFLite antes de hacer uso de esta función:</p>
<pre><code class="language-python">from tflite_runtime.interpreter import Interpreter
</code></pre>
<p>A continuación, obtenemos información sobre el tensor de entrada del modelo recién cargado, utilizando la función <code>get_input_details</code>, y consultando la propiedad <code>shape</code> de dicha entrada. Esto nos devolverá en las variables <code>widght</code> y <code>height</code> los tamaños de imagen esperados por el modelo. A partir de ahora, puedes consultar la forma de trabajar con la API de Python a través de la <a href="https://www.tensorflow.org/lite/api_docs/python/tf/lite">documentación oficial</a>.</p>
<p>Utilizaremos esta información para redimensionar la imagen capturada de la cámara como paso previo a la invocación del modelo.</p>
<div class="admonition danger">
<p class="admonition-title">Tarea</p>
<p>Asegúrate de que ambas líneas (creación del intérprete e importación de bibliotecas) son correctas. Comprueba el correcto funcionamiento del código utilizando el modelo <code>mobilenet_v1_1.0_224_quant.tflite</code> como entrada al programa <code>classify_opencv.py</code> sobre tu Raspberry Pi. Si todo ha ido bien, deberías ver una ventana mostrando la salida de la cámara con cierta información sobreimpresionada, y para cada fotograma, el resultado de la inferencia a través de línea de comandos.</p>
</div>
<h4 id="inferencia">Inferencia</h4>
<p>En el bucle principal de captura, se invoca a la función <code>classify_image</code>. Esta es una función propia, que
recibe simplemente el intérprete TFLite construido y la imagen capturada, pero cuyo cuerpo
contiene cierta funcionalidad de interés:</p>
<pre><code class="language-python">## Invoke model and process output (quantization-aware).
def classify_image(interpreter, image, top_k=1):
  &quot;&quot;&quot;Returns a sorted array of classification results.&quot;&quot;&quot;
  set_input_tensor(interpreter, image)

  interpreter.invoke()

  output_details = interpreter.get_output_details()[0]
  output = np.squeeze(interpreter.get_tensor(output_details['index']))

  # If the model is quantized (uint8 data), then dequantize the results
  if output_details['dtype'] == np.uint8:
    scale, zero_point = output_details['quantization']
    output = scale * (output - zero_point)

  ordered = np.argpartition(-output, top_k)
  return [(i, output[i]) for i in ordered[:top_k]]
</code></pre>
<p>Observa que la función opera en varias fases. En primer lugar, se obtiene una referencia al tensor de entrada
del modelo (invocando a la función propia <code>set_input_tensor</code>). En este caso, se copia, elemento a elemento,
la imagen de entrada (<code>image</code>) a dicho tensor:</p>
<pre><code class="language-python">## Establish input tensor from an image (copying).
def set_input_tensor(interpreter, image):
  tensor_index = interpreter.get_input_details()[0]['index']
  input_tensor = interpreter.tensor(tensor_index)()[0]
  input_tensor[:, :] = image
</code></pre>
<p>Volviendo a la función <code>classify_image</code>, una vez copiada la entrada al tensor de entrada del modelo, se invoca al modelo TFLite (<code>interpreter.invoke()</code>). Este es el proceso de inferencia o aplicación del modelo, y su tiempo de respuesta es crítico.</p>
<p>Por último, se procesa la salida (tensor de salida). En caso de ser una salida cuantizada (esto es, el tipo de cada elemento del array de salida es <em>uint8_t</em>, veremos más sobre cuantización en futuros laboratorios), ésta debe procesarse de forma acorde a los parámetros de cuantización utilizados.</p>
<p>Al final, la función devolverá un array de tuplas con la posición/clase (<code>i</code>) y la probabilidad de pertenencia del objeto
observado a dicha clase (<code>output[i]</code>).</p>
<div class="admonition danger">
<p class="admonition-title">Tarea</p>
<p>Imprime por pantalla la información sobre el tensor de entrada y el tensor de salida y analiza la salida proporcionada.</p>
</div>
<p>Obsérvese que, de forma previa a la invocación del modelo, la imágen capturada se ha reescalado de forma acorde al tamaño
del tensor de entrada del modelo:</p>
<pre><code class="language-python">image = cv2.resize(frame, (224, 224), interpolation = cv2.INTER_AREA)
</code></pre>
<h4 id="postprocesamiento">Postprocesamiento</h4>
<p>Por último, el programa sobreimpresiona información sobre etiqueta de clasificación, probabilidad de pertenencia a la clase y 
tiempo de inferencia sobre el propio <em>frame</em>, mostrando la imagen resultante:</p>
<pre><code class="language-python">cv2.putText(frame, '%s %.2f\n%.1fms' % ( labels[label_id], prob, elapsed_ms ), bottomLeftCornerOfText, font, fontScale, fontColor, lineType)

cv2.imshow('image',frame)
</code></pre>
<h3 id="desarrollo-utilizando-c">Desarrollo utilizando C++</h3>
<p>El rendimiento es un factor determinante en aplicaciones <em>Edge computing</em>, por lo que resultará interesante disponer de una base desarrollada en C++ sobre la que trabajar para el ejemplo de clasificación.</p>
<p>El fichero <code>classification.cpp</code> proporciona un flujo de trabajo completo para realizar una clasificación de imágenes similar a la realizada anteriormente usando la API de Python. </p>
<p>El código desarrollado es similar, paso a paso, al descrito para Python, por lo que no se incidirá en los detalles más allá de la <a href="https://www.tensorflow.org/lite/api_docs/cc">API utilizada</a>:</p>
<h4 id="ficheros-de-cabecera">Ficheros de cabecera</h4>
<p>Incluiremos ficheros de cabecera genéricos, para OpenCV y para TFLite:</p>
<pre><code class="language-cpp">#include &lt;stdio.h&gt;

// Cabeceras TFLite 
#include &quot;tensorflow/lite/interpreter.h&quot;
#include &quot;tensorflow/lite/kernels/register.h&quot;
#include &quot;tensorflow/lite/model.h&quot;

// Cabeceras OpenCV
#include &lt;opencv2/core.hpp&gt;
#include &lt;opencv2/videoio.hpp&gt;
#include &lt;opencv2/imgproc.hpp&gt;
#include &lt;opencv2/highgui.hpp&gt;
#include &lt;iostream&gt;
#include &lt;fstream&gt;
#include &lt;stdio.h&gt;

// Otras cabeceras C++
#include &lt;vector&gt;
#include &lt;numeric&gt;      // std::iota
#include &lt;algorithm&gt;    // std::sort, std::stable_sort
</code></pre>
<h4 id="carga-del-modelo-desde-un-fichero">Carga del modelo desde un fichero</h4>
<p>En este ejemplo, se realiza la carga del modelo directamente desde un fichero en disco, utilizando la rutina <code>BuildFromFile</code>:</p>
<pre><code class="language-cpp">   // 1. Cargamos modelo desde un fichero.
   std::unique_ptr&lt;tflite::FlatBufferModel&gt; model = tflite::FlatBufferModel::BuildFromFile(&quot;../Modelos/mobilenet_v1_1.0_224_quant.tflite&quot;);

   if(!model){
        printf(&quot;Failed to mmap model\n&quot;);
        exit(0);
   }

   // 2. Construimos el iterprete TFLite.
   tflite::ops::builtin::BuiltinOpResolver resolver;
   std::unique_ptr&lt;tflite::Interpreter&gt; interpreter;
   tflite::InterpreterBuilder(*model.get(), resolver)(&amp;interpreter);

   // 3. Alojamos espacio para tensores.
   interpreter-&gt;AllocateTensors();
</code></pre>
<p>Observa que, en el anterior fragmento de código, además de la carga del modelo, se construye un intérprete utilizando la clase <code>InterpreterBuilder</code>, y se aloja espacio para los tensores necesarios para aplicar el modelo.</p>
<h4 id="caracaterizacion-de-tensores-de-entrada-y-salida">Caracaterización de tensores de entrada y salida</h4>
<p>Como hemos hecho en el código Python, será necesario realizar una caracterización de los tensores de entrada y salida. El primero, para copiar nuestra imagen capturada desde cámara; el segundo, para procesar la salida obtenida:</p>
<pre><code class="language-cpp">    // 4. Identificamos el tensor de entrada y de salida.
    int input_number  = interpreter-&gt;inputs()[0];
    uint8_t * input_tensor  = interpreter-&gt;typed_tensor&lt;uint8_t&gt;(input_number);

    int output_number = interpreter-&gt;outputs()[0];
    uint8_t * output_tensor = interpreter-&gt;typed_tensor&lt;uint8_t&gt;(output_number);
</code></pre>
<div class="admonition danger">
<p class="admonition-title">Tarea</p>
<p>¿Cuál es el índice de los tensores de entrada y salida generados? Coinciden con los observados al mostrar por pantalla la información sobre ellos en el código Python.</p>
</div>
<h4 id="invocacion-del-modelo-tflite">Invocación del modelo TFLite</h4>
<p>Tras comenzar con la captura de vídeo y redimensionar la imagen de entrada, copiaremos al tensor de entrada
la imagen capturada, pixel a pixel:</p>
<pre><code class="language-cpp">   // 8. Copiamos imagen al tensor de entrada.
   for (int i = 0; i &lt; 224*224; ++i) {
      input_tensor[3*i + 0] = frame.at&lt;cv::Vec3b&gt;(i)[0];
      input_tensor[3*i + 1] = frame.at&lt;cv::Vec3b&gt;(i)[1];
      input_tensor[3*i + 2] = frame.at&lt;cv::Vec3b&gt;(i)[2];
   }
</code></pre>
<p>A continuación, invocamos al modelo:</p>
<pre><code class="language-cpp">   // 9. Invocamos al modelo.
   if (interpreter-&gt;Invoke() != kTfLiteOk) {
     cerr &lt;&lt; &quot;Failed to invoke tflite!&quot;;
     exit(-1);
   }
</code></pre>
<h4 id="clasificacion-y-analisis-de-salida">Clasificación y análisis de salida</h4>
<p>El proceso de análisis de salida es ligeramente distinto al usado en Python, aunque sigue una filosofía similar. En primer lugar, analizamos el tensor de salida:</p>
<pre><code class="language-cpp">   // 10. Analizamos el tamaño del tensor de salida.
   TfLiteIntArray* output_dims = interpreter-&gt;tensor(output_number)-&gt;dims;
   auto output_size = output_dims-&gt;data[output_dims-&gt;size - 1];
   cout &lt;&lt; output_size &lt;&lt; endl;
</code></pre>
<p>Como en el caso de Python, en función de la cuantización de la salida, deberemos procesarla de forma acorde (trataremos la cuantización en futuros laboratorios). En cualquier caso, el array <code>logits</code> contiene la probabilidad de pertenencia a cada una de las 1001 clases disponibles. El código que se os proporciona ordena dicho array y muestra por pantalla la clase más probable, junto a su probabilidad asociada.</p>
<h4 id="compilacion-y-uso">Compilación y uso</h4>
<p>A continuación, compila y ejecuta el programa para validar su funcionamiento:</p>
<pre><code class="language-sh">g++ classification.cpp -ltensorflow-lite -lpthread -ldl `pkg-config --cflags --libs opencv4` -o classification.x

./classification.x
</code></pre>
<p>La salida está preparada para mostrar el código numérico de la clase detectada con mayor probabilidad, dicha probabilidad, y la descripción textual de la clase.</p>
<div class="admonition danger">
<p class="admonition-title">Tarea</p>
<p>Temporiza, utilizando la rutinas de la clase <code>chrono</code> de C++, el proceso de inferencia, y compáralo con el de la versión Python.</p>
</div>
<div class="admonition danger">
<p class="admonition-title">Tarea</p>
<p>De forma opcional, investiga cómo sobreimpresionar la información asociada al proceso de inferencia (clase, probabilidad y tiempo) de forma similar a cómo lo hicimos en Python.</p>
</div>
<h2 id="uso-de-google-coral">Uso de Google Coral</h2>
<h3 id="runtime-edgetpu">Runtime EdgeTPU</h3>
<p>Para poder usar la Google Coral con TFLite es necesario instalar la librería correspondiente (<code>libedgetpu</code>). Las fuentes de esta librería están disponibles el <a href="https://github.com/google-coral/libedgetpu">GitHub de Google-Coral</a> pero lo más sencillo es descargar binarios pre-compilados para <code>bookworm</code> y <code>arm64</code> (también denominado <code>aarch64</code>). </p>
<p>Instrucciones de instalación:</p>
<pre><code class="language-sh">$ wget https://github.com/feranick/libedgetpu/releases/download/16.0TF2.16.1-1/libedgetpu-dev_16.0tf2.16.1-1.bookworm_arm64.deb
$ wget https://github.com/feranick/libedgetpu/releases/download/16.0TF2.16.1-1/libedgetpu1-max_16.0tf2.16.1-1.bookworm_arm64.deb
$ sudo dpkg -i libedgetpu*
</code></pre>
<h3 id="uso-de-edgetpu-desde-python">Uso de EdgeTPU desde Python</h3>
<p>Para utilizar el acelerador Google Coral (que debe estar conectado a la Raspberry Pi), realizaremos ciertas modificaciones en 
el código, que en este caso son mínimas. </p>
<p>En primer lugar, añadiremos un <code>import</code> en nuestro fichero:</p>
<pre><code class="language-python">from tflite_runtime.interpreter import load_delegate
</code></pre>
<p>A continuación, reemplazaremos la construcción del intérprete por la especificación de una biblioteca delegada para realizar la inferencia sobre la Google Coral:</p>
<pre><code class="language-python">interpreter = Interpreter(args.model,
    experimental_delegates=[load_delegate('libedgetpu.so.1.0')])
</code></pre>
<p>Finalmente, será necesario aplicar un modelo especialmente compilado para la Google Coral. Normalmente, este modelo se obtiene utilizando el <a href="https://coral.withgoogle.com/docs/edgetpu/compiler/">compilador de la Edge TPU</a>, pero en este caso se descarga y proporciona mediante el script <code>download.sh</code>. El nombre del modelo es <code>mobilenet_v1_1.0_224_quant_edgetpu.tflite</code>.</p>
<p>Así, podremos ejecutar sobre la Edge TPU usando:</p>
<pre><code class="language-sh">sudo python3 classify_opencv.py \
  --model ../Modelos/mobilenet_v1_1.0_224_quant_edgetpu.tflite \
  --labels ../Modelos/labels_mobilenet_quant_v1_224.txt
</code></pre>
<p>Obviamente, para que el anterior comando tenga éxito, la Google Coral deberá estar conectada a la Raspberry Pi  y el usuario que lo ejecuta debe tener permisos para usar el dispositivo. Estos permisos se pueden otorgar mediante la siguiente secuencia de comandos:</p>
<pre><code class="language-sh">$ lsusb -d 18d1:9302
Bus 002 Device 003: ID 18d1:9302 Google Inc.
$ echo 'SUBSYSTEM==&quot;usb&quot;, ATTR{idVendor}==&quot;18d1&quot;, ATTR{idProduct}==&quot;9302&quot;, MODE=&quot;0666&quot;, GROUP=&quot;plugdev&quot;' | sudo tee /etc/udev/rules.d/99-edgetpu.rules
$ sudo udevadm control --reload-rules &amp;&amp; sudo udevadm trigger
</code></pre>
<div class="admonition warning">
<p class="admonition-title">Aviso</p>
<p>Es posible que para que el cambio surta efecto sea necesario salir y volver a entrar en la sesión.</p>
</div>
<div class="admonition danger">
<p class="admonition-title">Tarea</p>
<p>Compara los tiempos de ejecución de la inferencia utilizando el procesdor de propósito general frente al rendimiento utilizando la Google Coral. ¿Qué ganancia de rendimiento observas? Ajusta la configuración de la captura de imagenes para que la comparación sea lo más justa:</p>
</div>
<h3 id="uso-de-edgetpu-desde-c">Uso de EdgeTPU desde C++</h3>
<p>Para usar la Google Coral desde C++ es preciso realizar algunas modificaciones al código visto anteriormente ademas de incluir la correspondiente cabecera (<code>&lt;edgetpu.h&gt;</code>). Para más información consultar la <a href="https://coral.ai/docs/reference/cpp/edgetpu/">documentación de la API de <code>libedgetpu</code></a>.</p>
<h4 id="apertura-del-dispositivo">Apertura del dispositivo</h4>
<p>Antes de poder trabajar con la EdgeTPU es preciso abrir el dispositivo con la función <code>OpenDevice</code> de la clase <code>EdgeTpuManager</code>:</p>
<pre><code class="language-c++">    auto tpu_context = edgetpu::EdgeTpuManager::GetSingleton()-&gt;OpenDevice();
</code></pre>
<h4 id="registrar-el-manejador-de-edgetpu-en-el-resolutor">Registrar el manejador de EdgeTPU en el resolutor</h4>
<p>A continuación es preciso añadir un operador personalizado para que TensorFlow Lite lo reconozca:</p>
<pre><code class="language-c++">    tflite::ops::builtin::BuiltinOpResolver resolver;
    resolver.AddCustom(edgetpu::kCustomOp, edgetpu::RegisterCustomOp());
</code></pre>
<h4 id="vincular-el-contexto-de-edgetpu-con-el-interprete">Vincular el contexto de EdgeTPU con el intérprete</h4>
<p>Por último es necesario vincular el contexto del dispositivo abierto previamente con el intérprete de TFLite:</p>
<pre><code class="language-c++">    interpreter-&gt;SetExternalContext(kTfLiteEdgeTpuContext,tpu_context.get());
</code></pre>
<p>Por lo demás el código es esencialmente el mismo que anteriormente.</p>
<h4 id="compilacion">Compilación</h4>
<p>Para poder compilar el código es preciso enlazar con la libraría <code>libedgetpu</code>:</p>
<pre><code class="language-c++">g++ classification.cpp -o classification.x `pkg-config --cflags --libs opencv4` -ltensorflow-lite -ledgetpu -lpthread -ldl
</code></pre>
<div class="admonition danger">
<p class="admonition-title">Tarea</p>
<p>Compara las versiones C++ CPU vs Google Coral ¿Qué ganancia de rendimiento observas? Compara el resultado con el obtenido en Python.</p>
</div></div>
            </div>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script src="../../js/bootstrap.bundle.min.js"></script>
        <script>
            var base_url = "../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../js/base.js"></script>
        <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
        <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
        <script src="../../js/mathjax.js"></script>
        <script src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>
        <script src="../../search/main.js"></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
            </div>
            <div class="modal-body">
                <p>From here you can search these documents. Enter your search terms below.</p>
                <form>
                    <div class="form-group">
                        <input type="search" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results" data-no-results-text="No results found"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
